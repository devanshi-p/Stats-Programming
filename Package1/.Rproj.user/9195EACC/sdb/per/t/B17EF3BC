{
    "collab_server" : "",
    "contents" : "##################################\n## Function 1: QR decomposition ##\n##################################\n#' Perform QR decomposition on the matrix A.\n#'\n#' @param matrix a matrix of n x m\n#' @return A list with Q.transpose and R where Q is an orthogonal n x n matrix and R is an upper triangular n x m matrix\n#' @seealso \\code{\\link{nchar}} which this function wraps\n#' @export\n#' @examples\n#' myQR(A)\n\nmyQR <- function(A){\n\n  ## Perform QR decomposition on the matrix A\n  ## Input:\n  ## A, an n x m matrix\n\n  ########################\n  ## FILL IN CODE BELOW ##\n  ########################\n  n <- dim(A)[1]\n  m <- dim(A)[2]\n  R <- A\n  Q <- diag(rep(1,n))\n\n  for (k in 1:(m-1))\n  {\n    x = array(0,c(n,1))\n    x[k:n,1] = R[k:n,k]\n    s = -1 * sign(x[k,1])\n    v = x\n    v[k] = x[k] - s*norm(x, type=\"F\")\n    u = v / norm(v, type=\"F\")\n\n    R = R - 2 * (u %*%(t(u) %*% R))\n    Q = Q - 2 * (u %*% (t(u) %*% Q))\n  }\n\n\n  ## Function should output a list with Q.transpose and R\n  ## Q is an orthogonal n x n matrix\n  ## R is an upper triangular n x m matrix\n  ## Q and R satisfy the equation: A = Q %*% R\n  return(list(\"Q\" = t(Q), \"R\" = R))\n\n}\n\n###############################################\n## Function 2: Linear regression based on QR ##\n###############################################\n#' Perform the linear regression of Y on X.\n#'\n#' @param matrix X is an n x p matrix of explanatory variables\n#' @param matrix Y is an n dimensional vector of responses\n#' @return The least squares solution vector\n#' @seealso \\code{\\link{nchar}} which this function wraps\n#' @export\n#' @examples\n#' myLinearModel(X, Y)\nmyLinearModel <- function(X, Y){\n\n  ## Perform the linear regression of Y on X\n  ## Input:\n  ## X is an n x p matrix of explanatory variables\n  ## Y is an n dimensional vector of responses\n  ## Do NOT simulate data in this function. n and p\n  ## should be determined by X.\n  ## Use myQR inside of this function\n\n  ########################\n  ## FILL IN CODE BELOW ##\n  ########################\n  n <- nrow(X)\n  p <- ncol(X)\n\n  Z <- cbind(rep(1,n),X,Y)\n  L = myQR(Z)\n  R1 = L$R[1:(p+1), 1:(p+1)]\n  Y1 = L$R[1:(p+1), p+2]\n  beta_ls = solve(R1) %*% Y1\n\n  ## Function returns the 1 x (p + 1) vector beta_ls,\n  ## the least squares solution vector\n  return(beta_ls)\n\n}\n\n##################################\n## Function 3: PCA based on QR  ##\n##################################\n#' Perform PCA on the matrix A.\n#'\n#' @param matrix A, a square matrix\n#' @param integer numIter is the number of iterations\n#' @return A list with D and V where D is a vector of eigenvalues of A and V is the matrix of eigenvectors of A\n#' @seealso \\code{\\link{nchar}} which this function wraps\n#' @export\n#' @examples\n#' myEigen_QR(A, 100)\nmyEigen_QR <- function(A, numIter = 1000){\n\n  ## Perform PCA on matrix A using your QR function, myQRC.\n  ## Input:\n  ## A: Square matrix\n  ## numIter: Number of iterations\n\n  ########################\n  ## FILL IN CODE BELOW ##\n  ########################\n  r <- nrow(A)\n  c <- ncol(A)\n  V = matrix(rnorm(r*r), nrow=r)\n  for(i in 1:numIter)\n  {\n    L = myQR(V)\n    V = A %*% L$Q\n  }\n  L1 = myQR(V)\n  Q = L1$Q\n  R = L1$R\n\n  ## Function should output a list with D and V\n  ## D is a vector of eigenvalues of A\n  ## V is the matrix of eigenvectors of A (in the\n  ## same order as the eigenvalues in D.)\n  return(list(\"D\" = diag(R), \"V\" = Q))\n\n}\n###################################################################\n## Function 4: Linear regression based on QR (without intercept) ##\n###################################################################\n\nmyLM <- function(X, Y){\n\n  ## Perform the linear regression of Y on X\n  ## Input:\n  ## X is an n x p matrix of explanatory variables\n  ## Y is an n dimensional vector of responses\n  ## Use myQR (or myQRC) inside of this function\n\n  ########################\n  ## FILL IN CODE BELOW ##\n  ########################\n  n <- nrow(X)\n  p <- ncol(X)\n\n  Z <- cbind(X,Y)\n  L = myQR(Z)\n  R1 = L$R[1:p, 1:p]\n  Y1 = L$R[1:p, p+1]\n  beta_ls = solve(R1) %*% Y1\n\n\n  ## Function returns the least squares solution vector\n  return(beta_ls)\n\n}\n\n######################################\n## Function 5: Logistic regression  ##\n######################################\n\n## Expit/sigmoid function\nexpit <- function(x){\n  1 / (1 + exp(-x))\n}\n\n#' Perform the linear regression of Y on X.\n#'\n#' @param matrix X is an n x p matrix of explanatory variables\n#' @param matrix Y is an n dimensional vector of responses\n#' @return The logistic regression solution vector\n#' @seealso \\code{\\link{nchar}} which this function wraps\n#' @export\n#' @examples\n#' myLogistic(X, Y)\nmyLogistic <- function(X, Y){\n\n  ## Perform the logistic regression of Y on X\n  ## Input:\n  ## X is an n x p matrix of explanatory variables\n  ## Y is an n dimensional vector of binary responses\n  ## Use myLM (or myLMC) inside of this function\n\n  ########################\n  ## FILL IN CODE BELOW ##\n  ########################\n  n <- nrow(X)\n  p <- ncol(X)\n\n  beta <- matrix(rep(0, p), nrow = p)\n  epsilon <- 1e-6\n  repeat\n  {\n    eta <- X%*%beta\n    pr <- expit(eta)\n    w <- pr*(1-pr)\n    Z <- eta + (Y-pr)/w\n    sw <- sqrt(w)\n    mw <- matrix(sw, n, p)\n    X1 <- mw*X\n    Y1 <- sw*Z\n    beta_new <- myLM(X1, Y1)\n    err <- sum(abs(beta_new-beta))\n    beta <- beta_new\n    if (err<epsilon)\n      break\n  }\n\n\n  ## Function returns the logistic regression solution vector\n  beta\n\n}\n",
    "created" : 1510642401537.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2795448321",
    "id" : "B17EF3BC",
    "lastKnownWriteTime" : 1510674579,
    "last_content_update" : 1510674579988,
    "path" : "~/Package1/R/Regression.R",
    "project_path" : "R/Regression.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}